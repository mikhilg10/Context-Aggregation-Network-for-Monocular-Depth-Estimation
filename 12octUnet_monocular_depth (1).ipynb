{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RMCKDCih670"
   },
   "source": [
    "http://imagesci.ece.cmu.edu/files/paper/2019/PhaseCam_ICCP19.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFzMHdLI_-08"
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHP6fong-9og"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import random\n",
    "# import glob\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9E3HY4519euw",
    "outputId": "39978f9b-e1a7-4d5f-cf3e-559bf9df0401"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "# import torchsummary as summary\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5ALtM_olY2X"
   },
   "source": [
    "#DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpUVdwz5I2HM",
    "outputId": "19d5b0aa-a3c7-47c4-efff-87bd1c832a36"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUQNbXOHJIbB"
   },
   "outputs": [],
   "source": [
    "# from scipy.io import loadmat\n",
    "import h5py\n",
    "\n",
    "files = h5py.File('nyu_depth_v2_labeled.mat.1','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_wqgLroKXO2",
    "outputId": "29a8565a-679c-485f-eafc-1b3c130060c5"
   },
   "outputs": [],
   "source": [
    "# plt.show()\n",
    "# plt.imshow(files['depths'][1].transpose((1,0)))\n",
    "print(files['images'][0].shape)\n",
    "# print(files['images'][0].shape[0])\n",
    "np.min(files['depths'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(files['images'][0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbNBq0VRlhw6"
   },
   "source": [
    "#DATALOADER Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JULQ1tcINVK-"
   },
   "outputs": [],
   "source": [
    "class ImDataNYU(Dataset):\n",
    "    def calculate_mean(self, images):\n",
    "        mean_image = np.mean(images, axis=0)\n",
    "        return mean_image\n",
    "\n",
    "    def __init__(self, filename, Mode, rgb_transform = None, depth_transform = None):\n",
    "        f = h5py.File(filename, 'r')\n",
    "        images_data = f['images'][0:1449]\n",
    "        depths_data = f['depths'][0:1449]\n",
    "\n",
    "        if Mode == \"training\":\n",
    "            self.images = images_data[0:1150]\n",
    "            self.depths = depths_data[0:1150]\n",
    "        elif Mode == \"validation\":\n",
    "            self.images = images_data[1150:1350]\n",
    "            self.depths = depths_data[1150:1350]\n",
    "\n",
    "        elif Mode == \"test\":\n",
    "            self.images = images_data[1350:]\n",
    "            self.depths = depths_data[1350:]\n",
    "\n",
    "        self.rgb_transform = rgb_transform\n",
    "        self.depth_transform = depth_transform\n",
    "        self.mean_image = self.calculate_mean(images_data[0:1449])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        # image = (image - self.mean_image)/np.std(image)\n",
    "        image = image.transpose((2, 1, 0)) #Verticslly jacked image\n",
    "        #image = Image.fromarray(image)\n",
    "        if self.rgb_transform:\n",
    "            image = self.rgb_transform(image)\n",
    "\n",
    "        depth = self.depths[idx]\n",
    "        #depth = np.reshape(depth, (1, depth.shape[0], depth.shape[1]))\n",
    "        depth = depth.transpose((1, 0)) #Verticslly jacked depth\n",
    "        if self.depth_transform:\n",
    "            depth = self.depth_transform(depth)\n",
    "        #sample = {'image': image, 'depth': depth}\n",
    "        return image,depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qw42WIQHk3Qg"
   },
   "source": [
    "#Augmengt the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxmoEc_fdxhy"
   },
   "outputs": [],
   "source": [
    "#Original DATA\n",
    "dataset_location='nyu_depth_v2_labeled.mat.1'\n",
    "batch_size= 72\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 480 , 480\n",
    "rgb_transforms = transforms.Compose([ \n",
    "                                         transforms.ToPILImage(),\n",
    "                                         transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "                                         transforms.ToTensor()\n",
    "                                         ])\n",
    "depth_transforms = transforms.Compose([\n",
    "                                           transforms.ToPILImage(),\n",
    "                                           transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "                                           transforms.ToTensor()\n",
    "                                           ])\n",
    "\n",
    "og_data = ImDataNYU( dataset_location,Mode='training', rgb_transform = rgb_transforms, depth_transform = depth_transforms)\n",
    "# train_loader = torch.utils.data.DataLoader(og_data,batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyeICnHXiGNv"
   },
   "outputs": [],
   "source": [
    "#Vertical_Flip data\n",
    "\n",
    "Vflip_rgb_transforms = transforms.Compose([ \n",
    "                                         transforms.ToPILImage(),\n",
    "                                         transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "                                         transforms.RandomVerticalFlip(p=1.0),\n",
    "                                         transforms.ToTensor()\n",
    "                                         ])\n",
    "Vflip_depth_transforms = transforms.Compose([\n",
    "                                           transforms.ToPILImage(),\n",
    "                                           transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "                                           transforms.RandomVerticalFlip(p=1.0),\n",
    "                                           transforms.ToTensor()\n",
    "                                           ])\n",
    "\n",
    "Vflip_data = ImDataNYU( dataset_location,Mode='training', rgb_transform = Vflip_rgb_transforms, depth_transform = Vflip_depth_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLv3hkgylAO-"
   },
   "outputs": [],
   "source": [
    "#Horizontal_Flip data\n",
    "\n",
    "gauss_rgb_transforms = transforms.Compose([ \n",
    "                                         transforms.ToPILImage(),\n",
    "                                         transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "#                                          transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
    "                                         transforms.ToTensor()\n",
    "                                         ])\n",
    "g_depth_transforms = transforms.Compose([\n",
    "                                           transforms.ToPILImage(),\n",
    "                                           transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "#                                            transforms.RandomHorizontalFlip(p=1.0),\n",
    "                                           transforms.ToTensor()\n",
    "                                           ])\n",
    "\n",
    "gauss_data = ImDataNYU( dataset_location,Mode='training', rgb_transform = gauss_rgb_transforms, depth_transform = g_depth_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pjACaNOKUYU"
   },
   "outputs": [],
   "source": [
    "#rotn data\n",
    "rot_rgb_transforms = transforms.Compose([ \n",
    "                                         transforms.ToPILImage(),\n",
    "                                         transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "                                         transforms.RandomRotation(degrees=(45)),\n",
    "                                         transforms.ToTensor()\n",
    "                                         ])\n",
    "rot_depth_transforms = transforms.Compose([\n",
    "                                           transforms.ToPILImage(),\n",
    "                                           transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "                                           transforms.RandomRotation(degrees=(45)),\n",
    "                                           transforms.ToTensor()\n",
    "                                           ])\n",
    "\n",
    "rot_data = ImDataNYU( dataset_location,Mode='training', rgb_transform = rot_rgb_transforms, depth_transform = rot_depth_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qfPT_0Mkz3O"
   },
   "source": [
    "#Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CenterCrop(size=256)\n",
    "crop_rgb_transforms = transforms.Compose([ \n",
    "                                         transforms.ToPILImage(),\n",
    "                                         transforms.CenterCrop(size=256),\n",
    "                                         transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "                                         transforms.ToTensor()\n",
    "                                         ])\n",
    "crop_depth_transforms = transforms.Compose([\n",
    "                                           transforms.ToPILImage(),\n",
    "                                           transforms.CenterCrop(size=256),\n",
    "                                           transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "                                           transforms.ToTensor()\n",
    "                                           ])\n",
    "\n",
    "crop_data = ImDataNYU( dataset_location,Mode='training', rgb_transform = crop_rgb_transforms, depth_transform = crop_depth_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomEqualize(p=0.5)\n",
    "#CenterCrop(size=256)\n",
    "eq_rgb_transforms = transforms.Compose([ \n",
    "                                         transforms.ToPILImage(),\n",
    "#                                          transforms.RandomEqualize(p=1.0),\n",
    "                                         transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "                                         transforms.ToTensor()\n",
    "                                         ])\n",
    "eq_depth_transforms = transforms.Compose([\n",
    "                                           transforms.ToPILImage(),\n",
    "                                           transforms.CenterCrop(size=256),\n",
    "                                           transforms.Resize((IMAGE_WIDTH,IMAGE_HEIGHT)),\n",
    "                                           transforms.ToTensor()\n",
    "                                           ])\n",
    "\n",
    "eq_data = ImDataNYU( dataset_location,Mode='training', rgb_transform = crop_rgb_transforms, depth_transform = crop_depth_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlQo2x1GjPlz"
   },
   "outputs": [],
   "source": [
    "#Total data\n",
    "# total_data = torch.utils.data.ConcatDataset([og_data, rot_data ,gauss_data ])\n",
    "total_data = torch.utils.data.ConcatDataset([og_data,rot_data,crop_data ,Vflip_data , gauss_data ]) #Vflip_data , Hflip_data\n",
    "#total_data = torch.utils.data.ConcatDataset([og_data,Vflip_data])    #rot_data, ,Hflip_data\n",
    "train_loader = torch.utils.data.DataLoader(total_data,batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnV8Qm9Fc88I"
   },
   "outputs": [],
   "source": [
    "#crop_data ,rcr_data #Validation set\n",
    "validation_data = ImDataNYU( dataset_location,Mode='validation', rgb_transform = rgb_transforms, depth_transform = depth_transforms)\n",
    "val_loader = torch.utils.data.DataLoader(validation_data,batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBbKjzIaKNBy"
   },
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMI-p1hwIlXn"
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "  def __init__(self,in_chan,out_chan):\n",
    "    super(DoubleConv,self).__init__()\n",
    "    self.conv  =  nn.Sequential(\n",
    "        nn.Conv2d(in_chan,out_chan,kernel_size=3,padding=1),\n",
    "        nn.BatchNorm2d(out_chan),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_chan,out_chan,kernel_size=3,padding=1),\n",
    "        nn.BatchNorm2d(out_chan),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "  def forward(self,x):\n",
    "    x = self.conv(x)\n",
    "    return x\n",
    "    \n",
    "class UNetDepth(nn.Module):\n",
    "  def __init__(self,feature = 16*[32,64,128,256,512]):\n",
    "    super(UNetDepth,self).__init__()\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "    self.enc1 = DoubleConv(3,feature[0])\n",
    "    self.enc2 = DoubleConv(feature[0],feature[1])\n",
    "    self.enc3 = DoubleConv(feature[1],feature[2])\n",
    "    self.enc4 = DoubleConv(feature[2],feature[3])\n",
    "    self.enc5 = DoubleConv(feature[3],feature[4])\n",
    "    self.upsample1 = nn.ConvTranspose2d(in_channels=feature[4],out_channels=feature[3],kernel_size=2,stride=2)\n",
    "    self.upsample2 = nn.ConvTranspose2d(in_channels=feature[3],out_channels=feature[2],kernel_size=2,stride=2)\n",
    "    self.upsample3 = nn.ConvTranspose2d(in_channels=feature[2],out_channels=feature[1],kernel_size=2,stride=2)\n",
    "    self.upsample4 = nn.ConvTranspose2d(in_channels=feature[1],out_channels=feature[0],kernel_size=2,stride=2)\n",
    "    self.dec1 = DoubleConv(feature[4],feature[3])\n",
    "    self.dec2 = DoubleConv(feature[3],feature[2])\n",
    "    self.dec3 = DoubleConv(feature[2],feature[1])\n",
    "    self.dec4 = DoubleConv(feature[1],feature[0])\n",
    "    self.out  = nn.Conv2d(feature[0],1,kernel_size=1)\n",
    "    self.drop = nn.Dropout2d(p=0.2)\n",
    "\n",
    "  def forward(self,x):\n",
    "    #encoder\n",
    "                          #inchannel 3 in_image = 256 256\n",
    "    x1  = self.enc1(x)    #inchannel=3 outchannel=32\n",
    "    x1  = self.drop(x1)\n",
    "    x2  = self.pool(x1)   # 32 128 128\n",
    "    x2  = self.drop(x2)   ##########$################\n",
    "    x3  = self.enc2(x2)   # inchannel=32 outchannel=64\n",
    "    x4  = self.pool(x3)   # 64 64 64 \n",
    "    x4  = self.drop(x4)   #######################\n",
    "    x5  = self.enc3(x4)   # inchannel=64 outchannel=128\n",
    "    x6  = self.pool(x5)   #128 32 32\n",
    "    x7  = self.enc4(x6)   #inchannel=128 outchannel=256\n",
    "    x8  = self.pool(x7)   #256 16 16\n",
    "    x9  = self.enc5(x8)   #inchannel=256 outchannel=512\n",
    "    #print(\"x9\",x9.shape)\n",
    "    #decoder\n",
    "    x10 = torch.cat([self.upsample1(x9),x7],dim=1)\n",
    "    #print(\"x10\", x10.shape)\n",
    "    x11 = self.dec1(x10)\n",
    "    #print(\"x11\", x11.shape)\n",
    "    x12 = torch.cat([self.upsample2(x11),x5],dim=1)\n",
    "    #print(\"x12\", x12.shape)\n",
    "    x13 = self.dec2(x12)\n",
    "    x13 = self.drop(x13)  ###############\n",
    "    #print(\"x13\", x13.shape)\n",
    "    x14 = torch.cat([self.upsample3(x13),x3],dim=1)\n",
    "    x15 = self.dec3(x14)\n",
    "    #print(\"x15\", x15.shape)\n",
    "    x16 = torch.cat([self.upsample4(x15),x1],dim=1)\n",
    "#     x16 = self.drop(x16)   ################\n",
    "    x17 = self.dec4(x16)\n",
    "    #print(\"x17\", x17.shape)\n",
    "    out = self.out(x17)\n",
    "    #print(\"out\", out.shape)\n",
    "    return F.relu(out)\n",
    "\n",
    "model = UNetDepth().to(device)\n",
    "# model = torch.nn.DataParallel(net, device_ids=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7BShSdBIk-R"
   },
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "# summary(model,(3,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5l4dg4pQKIcT"
   },
   "source": [
    "#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ScaleInvariantLoss(output, depth):\n",
    "#     # di = output - target\n",
    "#     output = torch.log(output+1)\n",
    "#     target = torch.log(depth+1)\n",
    "#     di = torch.abs(depth - output)\n",
    "#     n = (IMAGE_HEIGHT*IMAGE_WIDTH)\n",
    "#     di2 = torch.pow(di, 2)\n",
    "#     fisrt_term = torch.sum(di2)/n\n",
    "#     second_term = torch.pow(torch.sum(di), 2)/ (n**2)\n",
    "#     loss = fisrt_term - second_term\n",
    "#     return loss.mean()\n",
    "\n",
    "def gradLoss(output, depth):\n",
    "    # di = grad(output - target)\n",
    "    di = torch.gradient(output-depth)\n",
    "    n = IMAGE_HEIGHT*IMAGE_WIDTH\n",
    "    fisrt_term = torch.sum(di[1])/np.sqrt(n)\n",
    "    second_term = torch.sum(di[2])/ np.sqrt(n)\n",
    "    loss = torch.abs(fisrt_term) + torch.abs(second_term)\n",
    "    return loss\n",
    "\n",
    "#loss_f = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bdwp0vEh7SH",
    "outputId": "a2e0173d-7c3a-4f03-a6ad-61a951cef745",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "epochs=20\n",
    "LEARNING_RATE = 1e-3 #1e-3\n",
    "MOMENTUM  = 1e-4\n",
    "WEIGHT_DECAY =1e-4 #1e-4\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE,momentum=MOMENTUM ,weight_decay=WEIGHT_DECAY)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "loss_f = nn.MSELoss()\n",
    "training_loss=[]\n",
    "validation_loss=[]\n",
    "for epoch in range(epochs):\n",
    "  loop = tqdm(enumerate(train_loader), total = len(train_loader),desc = f'Epoch [{epoch+1}/{epochs}]')\n",
    "  running_loss=0\n",
    "  for batch_idx, (image,depth) in loop:\n",
    "    image=image.to(device)\n",
    "    depth=depth.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output= model(image)\n",
    "    loss = torch.sqrt(loss_f(output,depth))  #+ (gradLoss(output,depth))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss+=loss\n",
    "  running_loss/=(len(train_loader))\n",
    "  training_loss.append(running_loss)\n",
    "  print (f'Epoch [{epoch+1}/{epochs}], TrainLoss: {running_loss:.4f}',end='')\n",
    "\n",
    "  with torch.no_grad():\n",
    "    running_val_loss=0\n",
    "    for val_image,val_depth in val_loader:\n",
    "      val_image=val_image.to(device)\n",
    "      val_depth=val_depth.to(device)\n",
    "      val_out = model(val_image)\n",
    "      val_loss =  torch.sqrt(loss_f(val_depth,val_out))  #+ (gradLoss(output,depth))\n",
    "      running_val_loss +=val_loss\n",
    "    running_val_loss/=(len(val_loader))\n",
    "    validation_loss.append(running_val_loss)\n",
    "    print(f'\\tValLoss_: {running_val_loss: .4f}')\n",
    "PATH = './UNetDepth21Nov.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6I3y0PCHy3U",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a= np.arange(epochs+1)\n",
    "# plt.plot(a,training_loss)\n",
    "# plt.plot(a,validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jd3mc_77YWZm"
   },
   "outputs": [],
   "source": [
    "PATH = './UNetDepth21Nov.pth'\n",
    "ff = UNetDepth().to(device)\n",
    "check = torch.nn.DataParallel(ff, device_ids=[0, 1, 2, 3])\n",
    "check.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXDXADnR6yOE"
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2,2)\n",
    "image_test =  files['images'][1440]\n",
    "depth_test =  files['depths'][1440]\n",
    "axarr[0,0].imshow(image_test.T)\n",
    "axarr[0,1].imshow(depth_test.T)\n",
    "# img=rgb_transforms(image_test.T)\n",
    "img=rgb_transforms(image_test.T)\n",
    "# dep=depth_transforms(Image.fromarray(np.uint8(depth_test.T)))\n",
    "dep=depth_transforms(depth_test.T)\n",
    "print(img.shape)\n",
    "img2=img.view(1, 3, IMAGE_HEIGHT,IMAGE_WIDTH)\n",
    "test_out = check(img2.to(device))\n",
    "axarr[1,1].imshow(test_out[0][0].detach().cpu().numpy(), interpolation=\"bilinear\")\n",
    "axarr[1,0].imshow(img.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = rcr_depth_transforms(files['depths'][1420].T)\n",
    "# plt.imshow(np.transpose(p[0].detach().cpu().numpy(), (0,1)), interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.sqrt(loss_f(dep.to(device),test_out))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_out[0][0].detach().cpu().numpy(), interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.log(test_out[0][0].detach().cpu().numpy()+1e-3)\n",
    "# plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FineNetwork(nn.Module):\n",
    "#     def __init__(self,init=True):\n",
    "#         super(FineNetwork,self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 63, kernel_size = 9, padding = 'same')\n",
    "#         self.pool = nn.MaxPool2d(2)\n",
    "#         self.conv2 = nn.Conv2d(64, 64, kernel_size = 5, padding = 'same')\n",
    "#         self.conv3 = nn.Conv2d(64, 1, kernel_size = 5, padding = 'same')\n",
    "        \n",
    "#     def forward(self,x,y):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = torch.cat((x,y),1)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "#         return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = FineNetwork().to(device)\n",
    "# fine = torch.nn.DataParallel(f1, device_ids=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Parameters\n",
    "# epochs=10\n",
    "# LEARNING_RATE = 1e-3 #1e-3\n",
    "# MOMENTUM  = 1e-4\n",
    "# WEIGHT_DECAY =1e-4 #1e-4\n",
    "# # optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE,momentum=MOMENTUM ,weight_decay=WEIGHT_DECAY)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# loss_f = nn.MSELoss()\n",
    "# training_loss=[]\n",
    "# validation_loss=[]\n",
    "# for epoch in range(epochs):\n",
    "#   loop = tqdm(enumerate(train_loader), total = len(train_loader),desc = f'Epoch [{epoch+1}/{epochs}]')\n",
    "#   running_loss=0\n",
    "#   for batch_idx, (image,depth) in loop:\n",
    "#     image=image.to(device)\n",
    "#     depth=depth.to(device)\n",
    "#     optimizer.zero_grad()\n",
    "#     output= fine(image , check(image))\n",
    "#     loss = torch.sqrt(loss_f(output,depth))  #+ (gradLoss(output,depth))\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     running_loss+=loss\n",
    "#   running_loss/=(len(train_loader))\n",
    "#   training_loss.append(running_loss)\n",
    "#   print (f'Epoch [{epoch+1}/{epochs}], TrainLoss: {running_loss:.4f}',end='')\n",
    "\n",
    "#   with torch.no_grad():\n",
    "#     running_val_loss=0\n",
    "#     for val_image,val_depth in val_loader:\n",
    "#       val_image=val_image.to(device)\n",
    "#       val_depth=val_depth.to(device)\n",
    "#       val_out = fine(val_image , check(val_image))\n",
    "#       val_loss =  torch.sqrt(loss_f(val_depth,val_out))  #+ (gradLoss(output,depth))\n",
    "#       running_val_loss +=val_loss\n",
    "#     running_val_loss/=(len(val_loader))\n",
    "#     validation_loss.append(running_val_loss)\n",
    "#     print(f'\\tValLoss_: {running_val_loss: .4f}')\n",
    "# PATH = './fine.pth'\n",
    "# torch.save(fine.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Unet_monocular_depth.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
